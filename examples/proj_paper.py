import pandas as pd
import pyspark.sql.functions as f
from pyspark import StorageLevel
import importlib
import class_sig_option
import funcs_utilities
importlib.reload(class_sig_option)
from class_sig_option import OptionSignal
import class_EAP_researcher
importlib.reload(class_EAP_researcher)
from class_EAP_researcher import EAPResearcher
import class_performance_analysis
importlib.reload(class_performance_analysis)
from class_performance_analysis import PerformanceAnalytics
from pyspark.conf import SparkConf
from pyspark.sql import SparkSession, Window, DataFrame
import class_sig_evaluator; importlib.reload(class_sig_evaluator)
from class_sig_evaluator import PortfolioAnalysis
import funcs_utilities as utils
import quantstats
import seaborn as sns
import matplotlib
matplotlib.rcParams['text.usetex'] = False


import class_sig_technical; importlib.reload(class_sig_technical)
import numpy as np
from class_sig_technical import TechnicalSignal
import optuna
import joblib
import class_data_manipulator
importlib.reload(class_data_manipulator)
from class_data_manipulator import DataManipulator
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
import datetime
from statsmodels.tsa.stattools import acf
from linearmodels import FamaMacBeth
import dataframe_image as dfi
import pandas as pd
from typing import List

# spark
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)
pd.set_option('display.expand_frame_repr', False)
spark_conf = SparkConf() \
        .set("spark.executor.memory", '60g') \
        .set("spark.driver.memory", "60g") \
        .set('spark.driver.maxResultSize', '2g')\
        .set('spark.driver.extraJavaOptions', '-Duser.timezone=UTC') \
        .set('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')
spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()
# fixed parameters
MILLION = 1000000
BILLION = 1000000000
ROUNDTO = 3
START = '2005-01-01'
END = '2020-12-31'
# security  data----------------------------------------
sec = spark.read.parquet('data/dsf_linked')\
    .filter((f.col('date') >= '1996-01-01') & (f.col('date') <= '2020-12-31')) \
    .filter((f.col('shrcd') == 10) | (f.col('shrcd') == 11))
sec = TechnicalSignal(sec, 'date', 'cusip')\
        .add_std('ret', -20, 0, 'ret_std').data
# X----------------------------------------
## sec vars
sec_vars = sec\
    .withColumn('mktcap', f.col('mktcap')/BILLION)\
    .withColumn('turnover', f.col('vol')/f.col('shrout'))\
    .withColumn('illiq', f.abs('ret')/f.col('vol_d')*MILLION) \
    .withColumn('illiq', f.mean('illiq').over(Window.partitionBy('cusip').orderBy('date').rowsBetween(-20, 0))) \
    .select('date', 'cusip', 'turnover',  'illiq', 'prc','vol_d','mktcap')

sec_vars2 = TechnicalSignal(sec, 'date', 'cusip')\
        .add_cumr('ret', -20, 0, 'cum_r1') \
        .add_cumr('ret', -251, -21, 'cum_r2').data\
        .select('date', 'cusip', 'ret_std', 'cum_r1', 'cum_r2')
## fundamental vars
fundq_vars = spark.read.parquet('data/fundq')\
    .filter((f.col('date') >= '1996-01-01') & (f.col('date') <= '2020-12-31')) \
    .select('date', 'cusip', 'BM')
sec_vars.join(fundq_vars, on = ['date', 'cusip']).describe().show()
## option vars
opt = spark.read.parquet('data/opprcd_1996_2020') \
    .withColumn('cp_flag', f.when(f.col('cp_flag') == 'c', 'C').otherwise(f.col('cp_flag')))\
    .filter((f.col('date') >= '1996-01-01') & (f.col('date') <= '2020-12-31')) \
    .join(sec, on=['date', 'cusip'])\
    .withColumn('rv-iv', f.col('ret_std')*np.sqrt(252/20) - f.col('impl_volatility'))\
    .withColumn('opt_prc', (f.col('best_bid') + f.col('best_offer')) / 2) \
    .withColumn('ks', f.col('strike_price')/f.col('prc')) \
    .withColumn('elasticity', f.abs('delta') * f.col('prc') / f.col('opt_prc'))
OTM_P = (f.col('cp_flag') == 'P') & ((f.col('strike_price') <= 0.95*f.col('prc')) | (f.col('strike_price') >= 0.8*f.col('prc')))
ATM_C = (f.col('cp_flag') == 'C') & ((f.col('strike_price') >= 0.95*f.col('prc')) | (f.col('strike_price') <= 1.05*f.col('prc')))
weight = 'opt_vold'
IV_skew_sig = OptionSignal(opt.filter(OTM_P | ATM_C), 'date', 'cusip', 'cp_flag', 'C', 'P', weight) \
    .gen_IV_skew('impl_volatility', 'ks', f'IV_skew|{weight}') \
    .select('date', 'cusip', f'IV_skew|{weight}_P_minus_C')
OS = OptionSignal(opt, 'date', 'cusip', 'cp_flag', 'C', 'P', weight)
IV_sig = OS.gen_IV('impl_volatility', f'IV|{weight}').select('date', 'cusip', f'IV|{weight}_avg', f'IV|{weight}_spread1')
VRP = OS.gen_rv_iv_spread('rv-iv', f'rv_iv_spread|{weight}').select('date', 'cusip', f'rv_iv_spread|{weight}_avg')
ks_sigs = []
weights = ['opt_vold', 'opt_vol_adj', 'open_interest_adj', None]
for weight in weights:
    OS = OptionSignal(opt, 'date', 'cusip', 'cp_flag', 'C', 'P', weight)
    ks = OS.gen_ks('ks', f'ks|{weight}').select('date', 'cusip', f'ks|{weight}_spread2')
    ks_sigs.append(ks)
opt_sigs = [IV_skew_sig, IV_sig, VRP] + ks_sigs
opt_vars = utils.multi_join(opt_sigs, on=['date', 'cusip'])

## all vars
vars_daily = sec_vars\
    .join(sec_vars2, on= ['date', 'cusip'])\
    .join(fundq_vars, on=['date', 'cusip'])\
    .join(opt_vars, on=['date', 'cusip'])
# y-------------------------------------
sec_ret = TechnicalSignal(sec, 'date', 'cusip') \
        .add_cumr('ret', 1, 1, 'forward_r').data\
        .select('date', 'cusip', 'ret', 'forward_r')
# merge stock return with option signal
sec_ret.join(vars_daily, on=['date', 'cusip']).write.mode('overwrite').parquet(f'merged_daily_1996-01-01')

# read and resample data-------------------------------------
SIGS = ['ks|opt_vold_spread2', 'ks|opt_vol_adj_spread2',  'ks|open_interest_adj_spread2', 'ks|None_spread2']
FREQ = 'daily'
FORWORDR = 'forward_r'
RESAMPLED_R = 'ret'
MOM_FACTOR= False
SIGS =['ks|opt_vold_spread2', 'ks|open_interest_adj_spread2']
df_daily = pd.read_parquet(f'merged_daily_1996-01-01')
df_daily = df_daily.loc[ (df_daily['date'] >= START) & ((df_daily['date'] <= END))]
df_daily[FORWORDR] = df_daily[FORWORDR]*100
#df_monthly = utils.downsample_from_daily(df_daily, 'date', 'cusip', r='ret', vars=df_daily.iloc[:, 4:].columns.to_list(), to_freq='monthly',
                           #var_agg_rule='mean',
                           #resampled_r=RESAMPLED_R, resampled_forward_r=FORWORDR)

df = df_daily

# summarization--------------------------------------------------------------------------
## summarize sig
dfs = {'full': df,
       '2005-2009': df.loc[df['date'] <= '2009-12-31'],
       '2010-2015': df.loc[(df['date'] >= '2010-01-01') & (df['date'] <= '2015-12-31')],
       '2016-2020': df.loc[df['date'] >= '2016-01-01']}
for SIG in SIGS:
    summary_list = []
    summaries = pd.DataFrame()
    for key, value in dfs.items():
        eapr = EAPResearcher(value, 'date', 'cusip', SIG, FORWORDR)
        summary = eapr.summarize_sig(acf_nlags=10)
        summaries[key] = summary
    summary_list.append(summaries)
    summary_df = pd.concat(summary_list)
    print(f'Result for {SIG}\n', summary_df)
    with open(f'summary_sig_{SIG}.tex', 'w') as tex:
        tex.write(summary_df.round(2).to_latex())
# summarize vars
eapr = EAPResearcher(df, 'date', 'cusip', SIG, FORWORDR)
summary_vars = eapr.summarize_vars([SIG, 'mktcap', 'BM', 'turnover', 'illiq'])
print(summary_vars)
with open('summary_vars.tex', 'w') as tex:
    tex.write(summary_vars.round(4).to_latex())
df[SIGS].corr()
# portfolio analyais ---------------------------------------------------------------
covariates = ['mktcap', 'BM', 'ret_std','turnover', 'illiq', 'cum_r1', 'cum_r2',
                     'IV_skew|opt_vold_P_minus_C', 'IV|opt_vold_spread1',  'rv_iv_spread|opt_vold_avg']

for sig_name in SIGS:
    eapr = EAPResearcher(df, 'date', 'cusip', sig_name, forward_r = FORWORDR)
    port_r, sorted_res = eapr.uni_sort(spark, shift = 1, ntile=5,
                            weight = 'None',
                            covariates = covariates, freq=FREQ, mom=True, models = ['FF5', 'q-factor'])
    port_r = port_r.loc[port_r['sig_rank'] == 'high_minus_low'][['port_r']].reset_index()
    plt.clf()
    sns.lineplot(x = 'date', y = 'port_r',data = port_r)
    plt.xlabel("Year", fontsize=15)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.ylabel("%", fontsize=15)
    plt.savefig(f'long_short_{sig_name}.png')

    for row in [0, 2, 4]:
        sorted_res.iloc[row, :] = sorted_res.iloc[row, :]
    with open(f'sort_alpha_{sig_name}.tex', 'w') as tex:
        tex.write(sorted_res.round(3).to_latex())
    print(f'alphas for {sig_name} are:\n', sorted_res)
# Famaâ€“MacBeth Regressions-----------------------------------------------------------------
covariates_FM1 = []
covariates_FM2 = ['turnover', 'illiq', 'mktcap', 'ret_std', 'cum_r1', 'cum_r2', 'BM']
covariates_FM3 = ['turnover', 'illiq', 'mktcap', 'ret_std', 'cum_r1', 'cum_r2', 'BM',
                 'IV_skew|opt_vold_P_minus_C', 'IV|opt_vold_spread1',  'rv_iv_spread|opt_vold_avg']
for SIG in SIGS:
    eapr = EAPResearcher(df, 'date', 'cusip', SIG, forward_r = FORWORDR)
    res1 = eapr.FM_regression(covariates = covariates_FM1)
    res2 = eapr.FM_regression(covariates=covariates_FM2)
    res3 = eapr.FM_regression(covariates=covariates_FM3)
    FMs = pd.concat([res1, res2, res3], axis=1)
    FMs.round(3).to_latex(f'FM_{SIG}.tex')
    print(f'FM regression results for {SIG} are:\n',FMs)

# double sort----------------------------------------------------------------------------------------
eapr = EAPResearcher(df, 'date', 'cusip', 'ks|opt_vold_spread2', FORWORDR)
for var in ['mktcap', 'prc', 'illiq', 'BM', 'cum_r1', 'cum_r2']:
    double_sort = eapr.double_sort(spark, shift = 1,  freq = FREQ, forward_r=FORWORDR, sig_ntile=10, ntile2=3,sort_var2=var,
                                   dependent_sort=True, mom=True, models = ['FF5', 'q-factor'])
    sort_list= []
    for i in ['hedged_return', 'FF5-alpha', 'q-factor-alpha']:
        double_sort1 = double_sort['double_sorted'][i][['cond_rank', 'index',  'high_minus_low']]\
            .set_index(['cond_rank', 'index'])\
            .rename(columns = {'high_minus_low': i})
        sort_list.append(double_sort1)
    double_sort_df = pd.concat(sort_list, axis =  1)
    print(f'sorted by {var}', double_sort_df)
    double_sort_df.round(3).to_latex(f'double_sort_{var}.tex')

eapr.double_sort(spark, freq = FREQ, forward_r=FORWORDR, sig_ntile=5, ntile2=5,sort_var2='mktcap', dependent_sort=True)
# subgroup FM regression ----------------------------------------------------------------------------------------
for SIG in SIGS:
    n = 3
    eapr = EAPResearcher(df, 'date', 'cusip', SIG, forward_r=FORWORDR)
    results = []
    for var in ['mktcap', 'prc', 'illiq', 'BM', 'cum_r1', 'cum_r2']:
        res = eapr.double_sort_FM(covariates_FM3, sort_var=var, sort_n=3)
        res['var'] = var
        results.append(res)
    pd.concat(results, axis=1).round(2).to_latex(f'FM_subgroup_{SIG}.tex')
    print(f'subgroup FM for {SIG} is\n', pd.concat(results, axis=1))
# how long does predictability last?----------------------------------------------------------------
acf_list = []
for SIG in SIGS:
    alpha_list = []
    coef_list = []
    eapr = EAPResearcher(df, 'date', 'cusip', SIG, forward_r=f'forward_r')
    acfs = eapr.get_avg_acf(15, 'acf')
    print(f'acf for {SIG} is \n ', acfs)
    acf_list.append(acfs)
    pd.DataFrame(acf_list).transpose().round(3).to_latex(f'acf.tex')
    for i in [1, 2, 3, 4, 5, 10, 15]:

        df[f'forward_r{i}'] = df.sort_values('date').groupby('cusip')['ret'].shift(-i)
        eapr = EAPResearcher(df, 'date', 'cusip', SIG, forward_r=f'forward_r{i}')
        _, sorted_res= eapr.uni_sort(spark, ntile = 5, shift=i, weight = 'None', covariates = None, freq=FREQ)
        sorted_res.columns.name = ''
        sorted_res = sorted_res [['high_minus_low']].iloc[0:6,:].rename(columns = {'high_minus_low': 'day_'+str(i)})
        res = eapr.FM_regression(covariates=covariates_FM3)
        res = res.loc[res.index == SIG].rename(columns = {'parameter': 'day_'+str(i)})
        coef_list.append(res)
        alpha_list.append(sorted_res)
    alphas = pd.concat(alpha_list, axis =1)
    alphas.iloc[0:6:2] = alphas.iloc[0:6:2].apply(lambda x: x*100)
    alphas.round(3).to_latex(f'long_horizon_{SIG}.tex')
    coefs = pd.concat(coef_list, axis=1)
    print(f'Hedged returns and alphsa for {SIG} over long horizon is:\n', alphas)
    print(f'For {SIG}, The coefficient before the signal over long horizon is:\n', coefs)
# time series performance -----------------------------\
TRANSACTION_COST = True
SPY_daily = funcs_utilities.download_return('2005-01-01', '2020-12-31',  'SPY')[['Date','r']]\
    .assign(name = 'SPY').dropna()
perfs = []
extremes = []

for freq in ['daily', 'quarterly']:
    SPY_freq = \
        utils.downsample_from_daily(data=SPY_daily, time='Date', name='name', r='r', vars=[], to_freq=freq,
                                    var_agg_rule='last', resampled_r='r', resampled_forward_r='forward_SPY') \
            .set_index('Date')['r'] if freq != 'daily' else SPY_daily.set_index('Date')['r']
    rs = []
    turnovers = []
    for SIG in SIGS:
        # generate portfolio return
        df_freq = utils.downsample_from_daily(df_daily, 'date', 'cusip', r='ret', vars=[SIG],
                                              to_freq=freq, var_agg_rule='mean',
                                              resampled_r=RESAMPLED_R,
                                              resampled_forward_r=FORWORDR) if freq != 'daily' else df_daily
        df_freq.to_parquet('df_freq')
        pa = PortfolioAnalysis(spark.read.parquet('df_freq'), 'date', 'cusip', SIG, FORWORDR)
        port_r = pa.gen_portr(shift=1, ntile=10)
        r = port_r.loc[port_r['sig_rank'] == 'high_minus_low'].rename(columns = {'port_r': f'port_r_{SIG}'})[f'port_r_{SIG}']
        if TRANSACTION_COST is True:
            transaction_cost = pa.get_transact_cost(8.37/10000)
            turnover = transaction_cost.rename(columns = {'turnover': f'turnover_{SIG}'})[f'turnover_{SIG}']
            turnovers.append(turnover)
            r = transaction_cost\
                .rename(columns={'port_r_after_fee': f'port_r_after_fee_{SIG}'})[f'port_r_after_fee_{SIG}']

        rs.append(r)


        pa = PerformanceAnalytics(r)
        stats = pa.get_stats(freq=freq)
        stats['sig'] = SIG
        stats['freq'] = freq
        perfs.append(stats)

        pa = PerformanceAnalytics(SPY_freq.dropna())
        SPY_stats = pa.get_stats(freq=freq)
        SPY_stats['sig'] = 'SPY'
        SPY_stats['freq'] = freq
        perfs.append(SPY_stats)

        extreme = pd.DataFrame([r, SPY_freq]).transpose().dropna().sort_values('r').head(10).reset_index()
        extreme['sig'] = SIG
        extreme['freq'] = freq
        extremes.append(extreme)

    # plot turnover
    if TRANSACTION_COST is True:
        turnover_df = pd.concat(turnovers, axis = 1)
        summary1 = turnover_df.describe().transpose()[['mean', 'std']]
        summary2 = turnover_df.apply(lambda x: x.quantile([0.05, 0.25,  0.5, 0.75, 0.95])).transpose()
        pd.concat([summary1, summary2], axis=1).round(3).transpose().to_latex(f'turnover_{freq}.tex')

        appendix = '_after_fee'
    else:
        appendix=''

    rs_df = pd.concat(rs + [SPY_freq], axis = 1).apply(lambda x: (x+1).cumprod())
    sns.set_theme(style="white")
    plt.clf()
    sns.lineplot(data = rs_df)
    sns.despine()
    plt.legend(loc='upper left', labels=['Dollar volume weighted',
                                         'Open interest weighted',
                                                      'SPY'],
               frameon=False)
    #plt.title(f'Rebalance {freq.capitalize()}', fontdict={'fontsize': 25})
    x_max = rs_df.index.max()
    plt.xlabel("Year")
    plt.ylabel("Value in $")
    plt.tight_layout()
    plt.savefig(f'perf_{freq}{appendix}.png', dpi=300)

pd.concat(extremes, axis=1)\
    .apply(lambda x: x*100 if x.name in ['r', 'port_r_ks|opt_vold_spread2', 'port_r_ks|open_interest_adj_spread2'] else x)\
    .round(2).to_latex(f'extremes{appendix}.tex')
# stats
stats = pd.DataFrame(perfs)[['sig','freq', 'Annualized Geometric Return', 'Annual Volatility',
                                           'Annual Downside Deviation', 'Max Drawdown',
                                           'Annual Sharpe Ratio', 'Annual Sortino Ratio', 'Annual Calmer Ratio']]
stats[['Max Drawdown', 'Annualized Geometric Return', 'Annual Downside Deviation', 'Annual Volatility']] = \
    stats[['Max Drawdown', 'Annualized Geometric Return', 'Annual Downside Deviation', 'Annual Volatility']] * 100
stats.sort_values('freq', ascending=False).round(2).transpose().to_latex(f'perfs{appendix}.tex')
